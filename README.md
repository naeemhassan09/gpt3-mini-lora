# Mini-GPT-3 (<1 000 params) + LoRA (rank = 2)

Coursework for the MSc AI **Deep-Learning Module**  
*Goal*: reproduce LoRA on a toy GPT-3 architecture (< 1 000 learnable parameters) and compare to a fully-trainable baseline on the **MNLI** natural-language-inference task, all implemented in **Julia**.

---

## ðŸ—‚ Project structure


## âš¡ Quick start

```bash
git clone https://github.com/naeemhassan09/gpt3-mini-lora.git
cd gpt3-mini-lora
